{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we just want to use the lower layers of predefined models, and build our own layers on top of them.\n",
    "\n",
    "In chapter10_DNN we build a network of of two hidden layers. In this example we use the reuse hidden layer 1 but build a new hidden layer 2 one top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tensorflow and reset the default graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import mnist data, and define training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('datasets/mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load network from 'tmp/mnist_dnn_final.ckpt', and list all operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph('tmp/mnist_dnn_final.ckpt.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "DNN/hidden1/MatMul\n",
      "DNN/hidden1/BiasAdd\n",
      "DNN/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "DNN/hidden2/MatMul\n",
      "DNN/hidden2/BiasAdd\n",
      "DNN/hidden2/Relu\n",
      "logits/kernel/Initializer/random_uniform/shape\n",
      "logits/kernel/Initializer/random_uniform/min\n",
      "logits/kernel/Initializer/random_uniform/max\n",
      "logits/kernel/Initializer/random_uniform/RandomUniform\n",
      "logits/kernel/Initializer/random_uniform/sub\n",
      "logits/kernel/Initializer/random_uniform/mul\n",
      "logits/kernel/Initializer/random_uniform\n",
      "logits/kernel\n",
      "logits/kernel/Assign\n",
      "logits/kernel/read\n",
      "logits/bias/Initializer/zeros\n",
      "logits/bias\n",
      "logits/bias/Assign\n",
      "logits/bias/read\n",
      "DNN/logits/MatMul\n",
      "DNN/logits/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "train/gradients/Shape\n",
      "train/gradients/Const\n",
      "train/gradients/Fill\n",
      "train/gradients/loss/loss_grad/Reshape/shape\n",
      "train/gradients/loss/loss_grad/Reshape\n",
      "train/gradients/loss/loss_grad/Shape\n",
      "train/gradients/loss/loss_grad/Tile\n",
      "train/gradients/loss/loss_grad/Shape_1\n",
      "train/gradients/loss/loss_grad/Shape_2\n",
      "train/gradients/loss/loss_grad/Const\n",
      "train/gradients/loss/loss_grad/Prod\n",
      "train/gradients/loss/loss_grad/Const_1\n",
      "train/gradients/loss/loss_grad/Prod_1\n",
      "train/gradients/loss/loss_grad/Maximum/y\n",
      "train/gradients/loss/loss_grad/Maximum\n",
      "train/gradients/loss/loss_grad/floordiv\n",
      "train/gradients/loss/loss_grad/Cast\n",
      "train/gradients/loss/loss_grad/truediv\n",
      "train/gradients/zeros_like\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "train/gradients/DNN/logits/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/DNN/logits/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/DNN/logits/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/DNN/logits/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/DNN/logits/MatMul_grad/MatMul\n",
      "train/gradients/DNN/logits/MatMul_grad/MatMul_1\n",
      "train/gradients/DNN/logits/MatMul_grad/tuple/group_deps\n",
      "train/gradients/DNN/logits/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/DNN/logits/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/DNN/hidden2/Relu_grad/ReluGrad\n",
      "train/gradients/DNN/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/DNN/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/DNN/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/DNN/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/DNN/hidden2/MatMul_grad/MatMul\n",
      "train/gradients/DNN/hidden2/MatMul_grad/MatMul_1\n",
      "train/gradients/DNN/hidden2/MatMul_grad/tuple/group_deps\n",
      "train/gradients/DNN/hidden2/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/DNN/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/DNN/hidden1/Relu_grad/ReluGrad\n",
      "train/gradients/DNN/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/DNN/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/DNN/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/DNN/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/DNN/hidden1/MatMul_grad/MatMul\n",
      "train/gradients/DNN/hidden1/MatMul_grad/MatMul_1\n",
      "train/gradients/DNN/hidden1/MatMul_grad/tuple/group_deps\n",
      "train/gradients/DNN/hidden1/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/DNN/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "train/training_op/learning_rate\n",
      "train/training_op/update_hidden1/kernel/ApplyGradientDescent\n",
      "train/training_op/update_hidden1/bias/ApplyGradientDescent\n",
      "train/training_op/update_hidden2/kernel/ApplyGradientDescent\n",
      "train/training_op/update_hidden2/bias/ApplyGradientDescent\n",
      "train/training_op/update_logits/kernel/ApplyGradientDescent\n",
      "train/training_op/update_logits/bias/ApplyGradientDescent\n",
      "train/training_op\n",
      "eval/InTopK\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "eval/accuracy_1/tags\n",
      "eval/accuracy_1\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tensors of X, y, hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "hidden1 = tf.get_default_graph().get_tensor_by_name('DNN/hidden1/Relu:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define new hidden2 of 50 neurons and other parts of the network for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden2 = 50\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('new_DNN'):\n",
    "    new_hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='new_hidden2')\n",
    "    new_logits = tf.layers.dense(new_hidden2, n_output, activation=None, name='new_logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('new_loss'):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(entropy, name='new_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('new_eval'):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accruacy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('new_train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss, name='training_op')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights from 'tmp/mnist_dnn_final.ckpt', initialize new variables, and train another 20 epoches at batch size 50. Print out the validation accuracy after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_epoch = 20\n",
    "batch_size = 50\n",
    "n_step = mnist.train.num_examples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/mnist_dnn_final.ckpt\n",
      "Initial validation accuracy: 0.0822\n",
      "Epoch: 0, validation accuracy: 0.9674\n",
      "Epoch: 1, validation accuracy: 0.9766\n",
      "Epoch: 2, validation accuracy: 0.9772\n",
      "Epoch: 3, validation accuracy: 0.9814\n",
      "Epoch: 4, validation accuracy: 0.9784\n",
      "Epoch: 5, validation accuracy: 0.9808\n",
      "Epoch: 6, validation accuracy: 0.9800\n",
      "Epoch: 7, validation accuracy: 0.9806\n",
      "Epoch: 8, validation accuracy: 0.9782\n",
      "Epoch: 9, validation accuracy: 0.9794\n",
      "Epoch: 10, validation accuracy: 0.9810\n",
      "Epoch: 11, validation accuracy: 0.9816\n",
      "Epoch: 12, validation accuracy: 0.9834\n",
      "Epoch: 13, validation accuracy: 0.9808\n",
      "Epoch: 14, validation accuracy: 0.9806\n",
      "Epoch: 15, validation accuracy: 0.9812\n",
      "Epoch: 16, validation accuracy: 0.9814\n",
      "Epoch: 17, validation accuracy: 0.9806\n",
      "Epoch: 18, validation accuracy: 0.9782\n",
      "Epoch: 19, validation accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize all variables\n",
    "    saver.restore(sess, 'tmp/mnist_dnn_final.ckpt') # restore variables \n",
    "    accuracy_val = sess.run(accruacy, feed_dict={X:X_val, y:y_val})\n",
    "    print(\"Initial validation accuracy: {:.4f}\".format(accuracy_val))\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        for step in range(n_step):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = sess.run(accruacy, feed_dict={X:X_val, y:y_val})\n",
    "        print(\"Epoch: {}, validation accuracy: {:.4f}\".format(epoch, accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
