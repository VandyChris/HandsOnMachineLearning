{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter11_3, we load the parameters from the pre-trained model and continue to train the entire model. In some cases we want to \"freeze\" these parameters if we just consider the lower layers as well-trained feature extractor. \n",
    "\n",
    "In this example, we defined our own DNN network of five hidden layers, then load the weights up to the second hidden layer from tmp/mnist_dnn_final.ckpt. We freeze the first 2 layers and train the rest of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import numpy and tensorflow and reset the default graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load mnist data and get X_train, y_train, X_val, and y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/minst/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/minst/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/minst/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/minst/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('datasets/minst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run next cell to define our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 100  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused, frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused, frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # new\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the frozen layers won't change, it is possible to cache the output of the topmost frozen layer for each training instance. Since training goes through the whole dataset many times, this will give you a huge speed boost as you will only need to go through the frozen layers once per training instances, instead of once per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run another 20 epoches and print out the validation accuracy. Also print out the weight of hidden 1 before and after training to prove that the weight is frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[12]\")\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 20\n",
    "batch_size = 50\n",
    "n_step = mnist.train.num_examples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weight of hidden 1\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/mnist_dnn_final.ckpt\n",
      "[[ 0.00741297  0.04399479 -0.07298389 ...  0.06380799 -0.01855266\n",
      "   0.06487764]\n",
      " [-0.00944764  0.05897304  0.04332318 ... -0.06645274  0.02436784\n",
      "  -0.01333787]\n",
      " [ 0.00074603  0.01887694 -0.00775586 ...  0.00511462 -0.01614702\n",
      "  -0.05381356]\n",
      " ...\n",
      " [ 0.00572163 -0.01311519 -0.05707078 ... -0.05213419  0.05671324\n",
      "  -0.07045254]\n",
      " [-0.04161249  0.05519348 -0.04053325 ...  0.01326139  0.06936294\n",
      "   0.05498397]\n",
      " [-0.02907837 -0.01657688 -0.05163883 ... -0.05350867 -0.05564026\n",
      "  -0.04848889]]\n",
      "Epoch: 0, validation accuracy: 0.7596\n",
      "Epoch: 1, validation accuracy: 0.8478\n",
      "Epoch: 2, validation accuracy: 0.8814\n",
      "Epoch: 3, validation accuracy: 0.8968\n",
      "Epoch: 4, validation accuracy: 0.9038\n",
      "Epoch: 5, validation accuracy: 0.9078\n",
      "Epoch: 6, validation accuracy: 0.9116\n",
      "Epoch: 7, validation accuracy: 0.9124\n",
      "Epoch: 8, validation accuracy: 0.9134\n",
      "Epoch: 9, validation accuracy: 0.9154\n",
      "Epoch: 10, validation accuracy: 0.9178\n",
      "Epoch: 11, validation accuracy: 0.9214\n",
      "Epoch: 12, validation accuracy: 0.9206\n",
      "Epoch: 13, validation accuracy: 0.9204\n",
      "Epoch: 14, validation accuracy: 0.9220\n",
      "Epoch: 15, validation accuracy: 0.9218\n",
      "Epoch: 16, validation accuracy: 0.9226\n",
      "Epoch: 17, validation accuracy: 0.9260\n",
      "Epoch: 18, validation accuracy: 0.9256\n",
      "Epoch: 19, validation accuracy: 0.9254\n",
      "[[ 0.00741297  0.04399479 -0.07298389 ...  0.06380799 -0.01855266\n",
      "   0.06487764]\n",
      " [-0.00944764  0.05897304  0.04332318 ... -0.06645274  0.02436784\n",
      "  -0.01333787]\n",
      " [ 0.00074603  0.01887694 -0.00775586 ...  0.00511462 -0.01614702\n",
      "  -0.05381356]\n",
      " ...\n",
      " [ 0.00572163 -0.01311519 -0.05707078 ... -0.05213419  0.05671324\n",
      "  -0.07045254]\n",
      " [-0.04161249  0.05519348 -0.04053325 ...  0.01326139  0.06936294\n",
      "   0.05498397]\n",
      " [-0.02907837 -0.01657688 -0.05163883 ... -0.05350867 -0.05564026\n",
      "  -0.04848889]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    restore_saver.restore(sess, 'tmp/mnist_dnn_final.ckpt')\n",
    "    \n",
    "    # print the weight before training\n",
    "    print(sess.run(assign_kernel.inputs[0])) \n",
    "    \n",
    "    # cache the output of hidden layer 2\n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: mnist.train.images})\n",
    "    h2_cache_val = sess.run(hidden2, feed_dict={X: mnist.validation.images})\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        shuffle_idx = np.random.permutation(mnist.train.num_examples)\n",
    "        hidden2_batch_list = np.array_split(h2_cache[shuffle_idx], n_step)\n",
    "        y_batch_list = np.array_split(y_train[shuffle_idx], n_step)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batch_list, y_batch_list):\n",
    "            sess.run(training_op, feed_dict = {hidden2:hidden2_batch, y:y_batch})\n",
    "            \n",
    "        # accuracy_val = sess.run(accuracy, feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "        accuracy_val = sess.run(accuracy, feed_dict={hidden2: h2_cache_val, y: mnist.validation.labels})\n",
    "        print(\"Epoch: {}, validation accuracy: {:.4f}\".format(epoch, accuracy_val))\n",
    "    \n",
    "    # print the weight again. It should be unchanged since we froze the first 2 layers.\n",
    "    print(sess.run(assign_kernel.inputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
